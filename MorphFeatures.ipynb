{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8580d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aeon.datasets import load_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from source.models import Models\n",
    "from source.morph2 import Morph\n",
    "\n",
    "import pycatch22\n",
    "import tsfel\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac9c27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG_datasets = ['Epilepsy', 'ECG200','EOGHorizontalSignal', 'EOGVerticalSignal',\n",
    "        'CinCECGTorso', \n",
    "        'ECG5000', 'ECGFiveDays', 'StandWalkJump', \n",
    "        'TwoLeadECG','NerveDamage' , 'MedicalImages',\n",
    "        'Colposcopy',\n",
    "        'EyesOpenShut', 'ToeSegmentation1', 'Heartbeat',\n",
    "         'EMOPain', 'HandMovementDirection']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee68076",
   "metadata": {},
   "source": [
    "**Is it mandatory to pass the sampling rate?**\n",
    "\n",
    "No. However, please note that some spectral features rely on the sampling rate to be calculated correctly. Therefore, if you have access to the sampling rate, it is a good practice to pass the correct value. The default sampling rate value is set to 100 Hz. In case you do not have access to the sampling rate, you might refrain from using spectral features.\n",
    "\n",
    "\n",
    "**Does TSFEL allow to extract features from multi-dimensional time series with variable lengths?**\n",
    "\n",
    "Yes, it is possible, indeed. That’s actually one of the functionalities that weren’t adequately addressed by similar packages when we started the development of TSFEL. I recommend that the time series be stored in a data file and processed using the dataset_features_extractor. We are still updating the documentation of this functionality. In the meantime, you can read Section 2.2.1. Data ingestion and preprocessing of TSFEL publication which addresses that topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "443a3a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Dataset Name: Epilepsy\n",
      "--------------------------------------------------\n",
      "X_train Shape: (220, 3, 206)\n",
      "X_test Shape: (55, 3, 206)\n",
      "y_train Counts: (array([0, 1, 2, 3], dtype=int64), array([54, 58, 47, 61], dtype=int64))\n",
      "y_test Counts: (array([0, 1, 2, 3], dtype=int64), array([14, 15, 13, 13], dtype=int64))\n",
      "--------------------------------------------------\n",
      "{'lstm': 0.7454545454545455, 'catch22': 0.9818181818181818, 'rocket': 0.9818181818181818}\n",
      "--------------------------------------------------\n",
      "Processing Class: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:04<00:00, 12.97it/s]\n",
      "100%|██████████| 55/55 [00:20<00:00,  2.69it/s]\n",
      "100%|██████████| 55/55 [02:10<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Class 0 run time: 155.04727935791016\n",
      "epilepsy\n",
      "--------------------------------------------------\n",
      "Top 5 Feature Changes:\n",
      "1_Spectral distance: 3789.1613488213534\n",
      "2_Spectral distance: -3785.2938684010624\n",
      "0_Spectral distance: -948.348100859751\n",
      "1_MFCC_8: -322.0546937757657\n",
      "2_MFCC_8: -299.0308266675303\n",
      "--------------------------------------------------\n",
      "Top 5 Feature Changes:\n",
      "0_Spectral distance: 1458.4335271290618\n",
      "2_Spectral distance: 703.0004629257138\n",
      "1_Spectral distance: 447.6202438563409\n",
      "1_MFCC_8: -143.76735985764307\n",
      "2_MFCC_8: -114.20018993042413\n",
      "--------------------------------------------------\n",
      "Top 5 Feature Changes:\n",
      "0_Spectral distance: 1212.3671650812419\n",
      "2_Spectral distance: 597.8272296233586\n",
      "1_MFCC_8: -92.75892204541431\n",
      "1_MFCC_5: -79.03854252096039\n",
      "1_MFCC_3: 77.92951908253636\n",
      "Processing Class: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:05<00:00, 10.16it/s]\n",
      "100%|██████████| 55/55 [00:44<00:00,  1.24it/s]\n",
      "100%|██████████| 55/55 [01:34<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Class 1 run time: 144.00358033180237\n",
      "running\n",
      "--------------------------------------------------\n",
      "Top 5 Feature Changes:\n",
      "0_Spectral distance: -4414.47127658862\n",
      "1_Spectral distance: -3232.0596417376078\n",
      "2_Spectral distance: 3027.7909975876405\n",
      "0_MFCC_10: 301.9401321036814\n",
      "2_MFCC_10: 279.0184763410341\n",
      "--------------------------------------------------\n",
      "Top 5 Feature Changes:\n",
      "1_Spectral distance: -2872.9643556008555\n",
      "0_Spectral distance: -2132.267903967543\n",
      "2_Spectral distance: 1235.6357609493868\n",
      "0_MFCC_10: 340.8268723627563\n",
      "1_MFCC_10: 265.7234979801648\n",
      "--------------------------------------------------\n",
      "Top 5 Feature Changes:\n",
      "1_Spectral distance: -2289.975840171555\n",
      "0_Spectral distance: -1239.0373900615332\n",
      "2_Spectral distance: 943.9005885813044\n",
      "0_MFCC_10: 338.20051109612194\n",
      "1_MFCC_10: 229.4497500959704\n",
      "Processing Class: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:06<00:00,  8.77it/s]\n",
      "100%|██████████| 55/55 [00:21<00:00,  2.60it/s]\n",
      "100%|██████████| 55/55 [01:21<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Class 2 run time: 108.71743893623352\n",
      "sawing\n",
      "--------------------------------------------------\n",
      "Top 5 Feature Changes:\n",
      "2_Spectral distance: 2055.245120958615\n",
      "1_Spectral distance: -840.836865734208\n",
      "0_Spectral distance: 760.5335585526536\n",
      "0_MFCC_8: 288.2147401928687\n",
      "0_MFCC_3: 218.61812155903965\n",
      "--------------------------------------------------\n",
      "Top 5 Feature Changes:\n",
      "1_Spectral distance: 801.6559154183456\n",
      "0_Spectral distance: 797.5602290279326\n",
      "2_Spectral distance: -342.5892396847752\n",
      "0_MFCC_4: 166.9996488967628\n",
      "0_MFCC_8: 106.2844531022495\n",
      "--------------------------------------------------\n",
      "Top 5 Feature Changes:\n",
      "0_Spectral distance: 1635.9298122878472\n",
      "1_Spectral distance: 870.4994402002752\n",
      "2_Spectral distance: 822.8856081614771\n",
      "0_MFCC_4: 181.68660487214657\n",
      "0_MFCC_8: 112.08912692223285\n",
      "Processing Class: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:06<00:00,  8.86it/s]\n",
      "100%|██████████| 55/55 [00:21<00:00,  2.61it/s]\n",
      " 11%|█         | 6/55 [00:10<01:23,  1.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 90\u001b[0m\n\u001b[0;32m     88\u001b[0m morphing \u001b[38;5;241m=\u001b[39m Morph(X_test, y_test, c)\n\u001b[0;32m     89\u001b[0m morphing\u001b[38;5;241m.\u001b[39mget_DTWGlobalBorderline(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 90\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmorphing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCalculateMorph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m end_class \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m run time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_class\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anton\\Desktop\\Research\\Morphing\\source\\morph2.py:145\u001b[0m, in \u001b[0;36mMorph.CalculateMorph\u001b[1;34m(self, models, granularity, verbose)\u001b[0m\n\u001b[0;32m    143\u001b[0m     pred,_ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(morphing)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     pred,_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmorphing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Ensure valid morphing pairs\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m source_c0_y \u001b[38;5;129;01mand\u001b[39;00m pred[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m target_c1_y:\n\u001b[0;32m    149\u001b[0m         \n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# Find where label changes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anton\\Desktop\\Research\\Morphing\\source\\models.py:226\u001b[0m, in \u001b[0;36mModels.predict\u001b[1;34m(self, X_test)\u001b[0m\n\u001b[0;32m    222\u001b[0m     y_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict_proba(catch22_test)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrocket\u001b[39m\u001b[38;5;124m'\u001b[39m): \n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# X_test = np.squeeze(X_test, axis=1) if len(X_test.shape) == 3 and X_test.shape[1] == 1 else X_test\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     X_test_transform  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrocket_kernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X_test_transform)\n\u001b[0;32m    228\u001b[0m     y_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecision_function(X_test_transform)\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\MIST\\lib\\site-packages\\sktime\\transformations\\base.py:630\u001b[0m, in \u001b[0;36mBaseTransformer.transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# if no vectorization needed, we call _transform directly\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vectorization_needed:\n\u001b[1;32m--> 630\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# otherwise we call the vectorized version of predict\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m=\u001b[39mX_inner, y\u001b[38;5;241m=\u001b[39my_inner)\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\MIST\\lib\\site-packages\\sktime\\transformations\\panel\\rocket\\_rocket.py:145\u001b[0m, in \u001b[0;36mRocket._transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    143\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    144\u001b[0m set_num_threads(n_jobs)\n\u001b[1;32m--> 145\u001b[0m t \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43m_apply_kernels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernels\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    146\u001b[0m set_num_threads(prev_threads)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\MIST\\lib\\site-packages\\numba\\core\\serialize.py:30\u001b[0m, in \u001b[0;36m_numba_unpickle\u001b[1;34m(address, bytedata, hashed)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Keep unpickled object via `numba_unpickle` alive.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m _unpickled_memo \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_numba_unpickle\u001b[39m(address, bytedata, hashed):\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Used by `numba_unpickle` from _helperlib.c\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m        unpickled object\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     key \u001b[38;5;241m=\u001b[39m (address, hashed)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def multivariate_featues(row, cfg): \n",
    "    feature_names = []  \n",
    "    row_features = []  \n",
    "\n",
    "    for var_idx, variate in enumerate(row):\n",
    "        features = tsfel.time_series_features_extractor(cfg, variate, verbose=0)\n",
    "        \n",
    "        modified_columns = [\n",
    "            col.replace('0_', f'{var_idx}_') if var_idx != 0 else col \n",
    "            for col in features.columns\n",
    "        ]\n",
    "        features.columns = modified_columns  \n",
    "\n",
    "        row_features.append(features)\n",
    "        feature_names.extend(modified_columns) \n",
    "    \n",
    "    combined_row_features = pd.concat(row_features, axis=1)\n",
    "    dataframe = pd.DataFrame(combined_row_features, columns=feature_names)\n",
    "    return dataframe\n",
    "\n",
    "    \n",
    "all_results = {}\n",
    "all_features = {}\n",
    "results_array = np.empty((0, 11)) \n",
    "\n",
    "for df_name in ECG_datasets:\n",
    "    try:\n",
    "        # Load Dataset ===================================\n",
    "        X, y = load_classification(df_name)\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    except Exception as e:\n",
    "        print(f'{df_name}: Dataset Not Available - {str(e)}')\n",
    "        continue\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "    print(\"Dataset Name:\", df_name)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Useful Information\n",
    "    ts_length = X_train.shape[2]\n",
    "    df_size = X_train.shape[0]\n",
    "    n_classes = len(np.unique(y))  \n",
    "    variates = X_train.shape[1]  \n",
    "    class_counts = Counter(y_train)\n",
    "    num_classes = len(class_counts)\n",
    "\n",
    "    print('X_train Shape:', X_train.shape)\n",
    "    print('X_test Shape:', X_test.shape)\n",
    "\n",
    "    print('y_train Counts:', np.unique(y_train, return_counts=True))\n",
    "    print('y_test Counts:', np.unique(y_test, return_counts=True))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # Train and Evaluate Models ===================================\n",
    "    start = time.time()\n",
    "    lstm = Models('lstm', X_train, y_train)\n",
    "    lstm.train_lstm()\n",
    "    catch = Models('catch22', X_train, y_train)\n",
    "    catch.train_catch22()\n",
    "    rocket = Models('rocket', X_train, y_train)\n",
    "    rocket.train_rocket()\n",
    "\n",
    "    models = [lstm, catch, rocket] \n",
    "    acc = {}\n",
    "    for m in models:\n",
    "        pred, _ = m.predict(X_test)\n",
    "        acc[m.model_name] = accuracy_score(pred, y_test)\n",
    "\n",
    "    print(acc)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    class_results = {}\n",
    "    features_results = {}\n",
    "    # Loop Through each Class ===================================\n",
    "    for c in np.unique(y):\n",
    "        start_class = time.time()\n",
    "        print(f'Processing Class: {c}')\n",
    "\n",
    "        # Calculate class percentage\n",
    "        class_perc = round(class_counts[c] / df_size, 3)\n",
    "\n",
    "        # Compute Morphing ===================================\n",
    "        morphing = Morph(X_test, y_test, c)\n",
    "        morphing.get_DTWGlobalBorderline(X_test.shape[0])\n",
    "        results = morphing.CalculateMorph(models)\n",
    "\n",
    "        end_class = time.time()\n",
    "        print(f'Total Class {c} run time: {end_class - start_class}')\n",
    "\n",
    "        class_decoed = le.inverse_transform([c])[0]\n",
    "        print(class_decoed)\n",
    "        class_results[class_decoed] = results\n",
    "\n",
    "\n",
    "        # Feature Extraction using TSFEL ===================================\n",
    "        # normalize features difference \n",
    "        models_features = {} \n",
    "        feature_names = None\n",
    "        cfg_file = tsfel.get_features_by_domain() \n",
    "        for m in models:\n",
    "            differences = []\n",
    "            data = results[m.model_name]\n",
    "            for i in range(len(data['morphs'])):\n",
    "\n",
    "                morph = data['morphs'][i]\n",
    "                change_inx = data['change_indice'][i]\n",
    "               \n",
    "                source = morph[0]\n",
    "                target = morph[change_inx]\n",
    "                \n",
    "                # Univariate Time Series ===================================\n",
    "                if morph.shape[1] == 1:\n",
    "                    source_featues = tsfel.time_series_features_extractor(cfg_file, source[0], verbose=0)\n",
    "                    target_features = tsfel.time_series_features_extractor(cfg_file, target[0], verbose=0)\n",
    "                    #print(source.shape, target.shape)\n",
    "\n",
    "                # Multivariate Time Series ===================================\n",
    "                else:\n",
    "                    source_featues = multivariate_featues(source, cfg_file)\n",
    "                    target_features = multivariate_featues(target, cfg_file)\n",
    "\n",
    "                if feature_names is None:\n",
    "                    feature_names = source_featues.columns\n",
    "                \n",
    "                source_featues = np.array(source_featues)\n",
    "                target_features = np.array(target_features)\n",
    "\n",
    "                differences.append(target_features - source_featues)\n",
    "\n",
    "            avg_diff = np.mean(differences, axis=0).flatten()\n",
    "            \n",
    "            diffs = {}\n",
    "            for i in range(len(avg_diff)):\n",
    "                diffs[feature_names[i]] = avg_diff[i]\n",
    "\n",
    "            models_features[m.model_name]  = sorted(diffs.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "            print(\"-\" * 50)\n",
    "            print('Top 5 Feature Changes:')\n",
    "            for feature, diff in models_features[m.model_name][:5]:\n",
    "                print(f'{feature}: {diff}')\n",
    "\n",
    "        features_results[class_decoed] = models_features\n",
    "     \n",
    "        # Append results to NumPy array\n",
    "        for model in results.keys():\n",
    "            data = results[model]['metrics']\n",
    "            line = np.array([[df_name, df_size, variates, ts_length, n_classes, class_decoed, class_perc, model, data['mean'], data['std'], acc[model]]])\n",
    "            results_array = np.vstack((results_array, line))\n",
    "        \n",
    "\n",
    "    # Save results for the current dataset\n",
    "    file_name = f'results/pickles/{df_name}.pkl'\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(class_results, f)\n",
    "\n",
    "    # Save features results for the current dataset\n",
    "    file_name = f'results/pickles/features_{df_name}.pkl'\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(features_results, f)\n",
    "\n",
    "    # Add results to all_results\n",
    "    all_results[df_name] = class_results\n",
    "    all_features[df_name] = features_results\n",
    "\n",
    "    # Clean ups\n",
    "    del models, lstm, catch, rocket, class_results, features_results, X_train, X_test, y_train, y_test\n",
    "\n",
    "    # Convert NumPy array to Pandas DataFrame\n",
    "    columns = ['dataset', 'df_size', 'n_variates','ts_length', 'n_classes', 'class', 'class_perc', 'model', 'mean', 'std', 'model_acc']\n",
    "    dataframe = pd.DataFrame(results_array, columns=columns)\n",
    "    # Save results to CSV\n",
    "    dataframe.to_csv('results/final_results2.csv', index=False)\n",
    "\n",
    "    # Save all results to a single pickle file\n",
    "    with open('results/pickles/final_results2.pkl', 'wb') as f:\n",
    "        pickle.dump(all_results, f)\n",
    "    \n",
    "    # Save all features to a single pickle file\n",
    "    with open('results/pickles/final_features2.pkl', 'wb') as f:\n",
    "        pickle.dump(all_features, f)\n",
    "\n",
    "print('All results saved successfully!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
